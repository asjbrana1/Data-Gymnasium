import matplotlib.pyplot as plt
# Firstly we will import the stock data for Apple from 03/20/2023 - 03/15/2024 and store it as a Pandas Dataframe object
import pandas as pd
df = pd.read_csv("AAPL.csv", index_col="Date", parse_dates=True, squeeze=True) #Setting the index column as the date

#Next we will use the ADTK toolkit to validate the imported data to ensure there are no missing values or errors
from adtk.data import validate_series
validate_df = validate_series(df)

# We will then split the dataframe into 2 different ones. Specifically, we will analyze the closing stock price and volume data
df_closing = df['Close']
df_volume = df['Volume']

#Next we plot and visualize these 2 separate time series
from adtk.visualization import plot
plot(df_closing)
plt.show()
plot(df_volume)
plt.show()

# The simplest test we can first execute is to use a threshold value to see on which days the stock went above or
# below a certain price
from adtk.detector import ThresholdAD
threshold_ad = ThresholdAD(high=195, low=170) #the high and low values of the threshold can bet set here
anomalies = threshold_ad.detect(df_closing)
#next we plot the results of the threshold analysis. The Anomalies are marked in red
plot(df_closing, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=5, anomaly_color='red', anomaly_tag="marker");
plt.show()
#
# We can also use quantiles to tag data points as anomalies.
from adtk.detector import QuantileAD
quantile_ad = QuantileAD(high=0.9, low=0.1) #sets the range of the quantiles
anomalies = quantile_ad.fit_detect(df_closing)
plot(df_closing, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=5, anomaly_color='red', anomaly_tag="marker");
plt.show()

# We can also utilize the inter quantile range to tag data points as anomalous. Lets do this for the volume of trades
from adtk.detector import InterQuartileRangeAD
iqr_ad = InterQuartileRangeAD(c=1.5)
anomalies = iqr_ad.fit_detect(df_volume)
plot(df_volume, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=5, anomaly_color='red', anomaly_tag="marker");
plt.show()
#
#These are very simple statistical models to identify anomalies in time series data
#Next we will move on to more sophisticated models. Starting with the generalized ESD test which uses
#the student's T-Distribution. One caveat to keep in mind is that this text assumes the data is normally
#distributed, which is often not the case for stock price and trading volume time series data.
#Lets us use ESD on the stock trade volume data, since it is more stable than the pricing
from adtk.detector import GeneralizedESDTestAD
esd_ad = GeneralizedESDTestAD(alpha=0.3) #Decreasing the alpha lowers the sensitivity and vice versa
anomalies = esd_ad.fit_detect(df_volume)
plot(df_volume, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=5, anomaly_color='red', anomaly_tag="marker");
plt.show()
#Decreasing the alpha lowers the sensitivity of the test and results in fewer points being tagged as anomalous
esd_ad = GeneralizedESDTestAD(alpha=0.01) #Decreasing the alpha lowers the sensitivity
anomalies = esd_ad.fit_detect(df_volume)
plot(df_volume, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_markersize=5, anomaly_color='red', anomaly_tag="marker");
plt.show()
# The difference from the changing the alpha is more pronounced the closer the data follows a normal distribution

#Next we will utilize a model that uses a double rolling aggregate called PersistAD.
#This model directly compares each time series value with its previous values
#We will first use this to identify large positive changes in stock price
from adtk.detector import PersistAD
persist_ad = PersistAD(c=1.0, side='positive') #side can be positive or negative. Increasing c, which is the threshold# will result in fewer points being identified as anamolous
anomalies = persist_ad.fit_detect(df_closing)
plot(df_closing, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red')
plt.show()
#We can also use this to identify large negative drops in prices
persist_ad = PersistAD(c=1.0, side='negative') #side can be positive or negative. Increasing c, which is the threshold
# will result in fewer points being identified as anamolous
anomalies = persist_ad.fit_detect(df_closing)
plot(df_closing, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red')
plt.show()
#Increasing the value of c, increases the threshold and will result in fewer points being identified as anomalous.
persist_ad = PersistAD(c=2.0, side='negative') #side can be positive or negative. Increasing c, which is the threshold
# will result in fewer points being identified as anomalous
anomalies = persist_ad.fit_detect(df_closing)
plot(df_closing, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red')
plt.show()

#Next we will implment Level Shift AD
#This model detects shift of value level by tracking the difference between median values at two sliding time windows next to each other.
#It is not sensitive to instantaneous spikes and could be a good choice if noisy outliers happen frequently.
#It also utilizes a double rolling aggregate
from adtk.detector import LevelShiftAD
level_shift_ad = LevelShiftAD(c=1.0, side='both', window=5)
anomalies = level_shift_ad.fit_detect(df_closing)
plot(df_closing, anomaly=anomalies, anomaly_color='red')
plt.show()
# As the chart shows, normal trends such as a continuous increase/decrease in closing price
# are not tagged as anomalous. However, a sudden shift from a previous upward or downward trends are.
# The window determines how far the in the past, in days, the model looks at before tagging a point as
# anomalous. Increasing the window is analogous to a smoothing process and will lead to a fewer set of
# points tagged as anomalies. Next we change the window from 5 to 10 and observe the stated outcome
level_shift_ad = LevelShiftAD(c=1.0, side='both', window=10)
anomalies = level_shift_ad.fit_detect(df_closing)
plot(df_closing, anomaly=anomalies, anomaly_color='red')
plt.show()

#To track volatility, we can use Volatility Shift AD
#This model detects shift of volatility level by tracking the difference between standard deviations at
#two sliding time windows next to each other
#the windows identify periods of time when the data points change consistently from the previous window
#we will run this models on the trading volume data
from adtk.detector import VolatilityShiftAD
volatility_shift_ad = VolatilityShiftAD(c=1.0, window=20)
anomalies = volatility_shift_ad.fit_detect(df_volume)
plot(df_volume, anomaly=anomalies, anomaly_color='red')
plt.show()

#We can also use auto-regression to identify changes in time series data
#Autoregression is a statistical technique used in time-series analysis
#that assumes that the current value of a time series is a function of its past values.
from adtk.detector import AutoregressionAD
autoregression_ad = AutoregressionAD(n_steps=10, step_size=5, c=1.0)
anomalies = autoregression_ad.fit_detect(df_volume)
plot(df_volume, anomaly=anomalies, ts_markersize=1, anomaly_color='red', anomaly_tag="marker", anomaly_markersize=2)
plt.show()

#We can also look at 2 time series of data (multi-variate time series) and see if anomalies exists based on them
#as an example we can take the closing stock prices on any given day and the
#volume of stock traded to analyze if there exists a relationship between the 2
#first lets place the data for the closing prices and volume into one dataframe object
df_close_volume = df[['Close','Volume']] #creating a new dataframe with closing price and volume data
print(df_close_volume)
#since the trading volume values are much larger than the close prices we will need to divide
#the volume column for visualization purposes. This will not affect the models in any way
#since we are scaling one set of values and not the other.
df_close_volume['Volume']= df_close_volume['Volume'].div(1000000) #appropriate scale
print(df_close_volume)
# We can then use a clustering algorithm, such as k-means to identify clusters of points
# where the closiing price and volume change in a correlated manner
from adtk.detector import MinClusterDetector
from sklearn.cluster import KMeans
min_cluster_detector = MinClusterDetector(KMeans(n_clusters=3)) #increasing the clusters increases the number of anomalous points
anomalies = min_cluster_detector.fit_detect(df_close_volume)
plot(df_close_volume, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red', anomaly_alpha=0.3, curve_group='all')
plt.show()
#
# We can also use regression to identify anomalies in multi-variate time series
from adtk.detector import RegressionAD
from sklearn.linear_model import LinearRegression
regression_ad = RegressionAD(regressor=LinearRegression(), target="Volume", c=3.0)
anomalies = regression_ad.fit_detect(df_close_volume)
plot(df_close_volume, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red', anomaly_alpha=0.3, curve_group='all')
plt.show()

#PcaAD performs principal component analysis (PCA) to multivariate time series (every time point as a vector in a high-dimensional space)
# and tracks reconstruction error of those vectors.
# This detector may be helpful when normal points are supposed to be at a lower-rank manifold while anomalous points are not
from adtk.detector import PcaAD
pca_ad = PcaAD(k=2)
anomalies = pca_ad.fit_detect(df_close_volume)
plot(df_close_volume, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color='red', anomaly_alpha=0.3, curve_group='all')
plt.show()































